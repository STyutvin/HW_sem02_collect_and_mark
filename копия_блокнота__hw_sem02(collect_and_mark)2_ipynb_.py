# -*- coding: utf-8 -*-
"""Копия блокнота "HW_sem02(collect_and_mark)2.ipynb"

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1OckQ8lCupvoUBui3w4TEwyUnPikU7wZq
"""

# Импорт необходимых библиотек
from bs4 import BeautifulSoup
import requests
import pandas as pd
import urllib.parse
import re
import json

# Путь к странице с данным
url = "https://books.toscrape.com/"

# Пустые списки, которые будут содержать соответствующие данные: название, цена, остаток экземпляров, описание
name_book = []
price = []
qty_availible = []
description = []

# Пустой словарь для записи результатов
output = {}

# Чтобы не перебирать все 50 страниц, возьмем четыре
count = 0
while count <= 3:
  # GET-запрос к серверу
  page = requests.get(url)

  # Парсинг данных
  soup = BeautifulSoup(page.content, 'html.parser')

  # Парсинг ссылки перехода на следующую страницу
  next_page_link = soup.find(class_="next").find("a").get("href")

  # Поиск по тегу <li>
  result = soup.find_all('li', ('class','col-xs-6 col-sm-4 col-md-3 col-lg-3'))

  # Объединение ссылок на книги
  # Первая часть ссылки
  url_1 = url

  # Извлечение списка относительных ссылок на товары
  url_2 = []
  for i in result:
    url_2.append(i.find('a').get('href'))

  # Объединение двух частей ссылки в абсолютный путь и создание списка со ссылками на каждый товар, расположенный на странице
  url_joined = []

  for link in url_2:
    url_joined.append(urllib.parse.urljoin(url_1, link))

  # Цикл для сбора требуемой информации
  for i in url_joined:
    response = requests.get(i)
    soup_2 = BeautifulSoup(response.content, 'html.parser')
    # Парсинг названия книги
    try:
      name_book.append(soup_2.find('h1').text)
    except:
      name_book.append('')
    # Парсинг оставшегося количества
    try:
      av = soup_2.find('p', ('class', 'instock availability')).text
      av = int(re.sub(r'[^\d.]+', '', av))
      qty_availible.append(av)
    except:
      qty_availible.append('')
    # Парсинг цены
    try:
      pr = soup_2.find('p', ('class', 'price_color')).text
      pr = float(re.sub(r'[^\d.]+', '', pr))
      price.append(pr)
    except:
      price.append('')
    # Парсинг описания
    try:
      description.append(soup_2.find('h2').find_next().text)
    except:
      description.append('')

    output = {'name_book': name_book, 'price': price, 'in_stock': qty_availible, 'description': description}

  # Условие выхода из цикла While-True
  if not next_page_link:
    break

  url = url + next_page_link
  count = count + 1

df = pd.DataFrame(output)
print(df)

""" 
Ошибка при выводе, и я не могу найти причину... Если выполнять строки кода отдельно в блокноте, то ссылка находится. Но в цикле почему-то нет.
Возможно при прохождении цикла теряется путь(запрос к сайту), и поэтому ссылка не находится. Хотя я пропобовал добавлять исходный url непосредственно
в тело цикла, но тогда не осуществляется переход по страницам.

File "d:\Geek_Brains\Сбор и разметка данных\Семинар 02\HW\копия_блокнота__hw_sem02(collect_and_mark)2_ipynb_.py", line 40, in <module>
    next_page_link = soup.find(class_="next").find("a").get("href")
                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'NoneType' object has no attribute 'find'"""